---
layout: post
title:  "TBM参数预测系列（零）——数据可视化后的初步想法"
date:   2019-02-19 22:32:04 +0800
categories: ML
---

这次比赛依托于吉林引松供水工程自流输水隧道的掘进记录，提出全断面隧道掘进机（Tunnel Boring Machine, TBM）挖掘参数和性能的两项预测回归问题和一项预警问题。

就数据本身来说，每秒产生一个数据点（共40.8亿组）、没有明显的数据缺失、包含198个设备传感器数据以及额外的对应于桩号的地质信息、经过昂贵的手工数据整理，是相当完整且高质量的数据集。但比赛是中铁装备主办的，作为传统的盾构装备国企，这个交叉学科竞赛的举办也存在不少缺点。例如主办方没有提供微信群外的论坛和讨论空间、数据用麻烦不断的百度网盘分发、没有Kaggle Kernel之类的展示平台，组织上显得不如纯粹的数据科学比赛充分。赛题包含于数据科学的范畴，但参赛的队伍却大多来自纯粹的土木工程背景。

# 赛题和数据

## 赛题
比赛主要有两个任务：预测稳定段控制参数和稳定段掘进参数。题目具体细节见[PPT(转成PDF)](/assets/files/TBM掘进参数数据分享与机器学习交底说明.pdf)和[Word文档(已转成PDF)](/assets/files/02TBM掘进参数数据分享与机器学习：平行分析研讨交流计划书)。

## 数据
设备数据包含802个CSV文件。每个CSV文件记录TBM一天的运行设备数据，每秒一个数据点，共八万多行（24h），包含198列设备数据。打印出第一天的最后几个数据点看看。

{% include tbm_table_columns.html %}

然后按列画一下第一天的数据。
![第一天图表](/assets/img/tbm1.png)

从图中可以看出，循环的趋势还是比较明显的，这第一天的数据有四个明显的循环。但比较烦人的是，赛题（见上小节）是以循环为单位做预测，而直接提供的数据中毫无关于分割“循环”的线索。那么数据处理的第一步，自然要求先把数据拆分成循环，并进一步拆分成空推段、上升段、稳定段。这个博客系列里的下一篇，将总结我是怎么这些数据的。


# Related Work
虽然大一是在土木工程专业读的，但对地下的“领域知识 (domain knowlege)”实在知之甚少，就连大一暑假参观隧道博物馆的专业实习也划水划了过去。幸运的是，深度学习的优势所在包括了无需掌握大量专业知识，也降低了特征工程的显著性。不论如何，还是稍微找了些有关文献。

中铁2018年的论文[*TBM掘进参数智能控制系统的研究与应用*](/assets/files/TBM掘进参数智能控制系统的研究与应用.pdf)介绍了中铁目前处理掘进参数预测采用的方法。数据处理方面，论文没有详细阐述循环分段的方法，只提到了*通过循环均值的方法得到掘进循环上升段与稳态段的分界点*。没有使用时间序列数据，*岩体信息感知模型*和*掘进参数预测模型*分别建立设备参数=>岩体信息和岩体信息=>掘进参数的映射关系，模型采用的是三种简单回归模型(FC、SVM、最小二乘回归模型)的ensemble。*岩体状态参数和 TBM掘进参数预测准确率均在 90%以上*。

找到的其他的几篇论文基本也采用全连接网络及其变种、SVM、随机森林等传统机器学习方法进行建模，A Machine Learning Approach to Predicting and Maximizing Penetration Rates in Earth Pressure Balance Tunnel Boring Machines先用随机森林挑选出重要特征，再用这些特征建立线性回归模型。Modeling Tunnel Boring Machine Performance by Neuro-Fuzzy Methods是较早的使用全连接神经网络的文章。TBM Performance Prediction in Rock Tunneling Using Various Artificial Intelligence Algorithms综述性比较强，对比了曾经进行TBM参数预测的工作所使用的输入和输出量，但没有比较好的总结各机器学习模型的利弊。

比较有意思的一篇是IMPROVED TUNNELING KNOWLEDGE THROUGH ROBUST MACHINE LEARNING ，James I. et al.的计算机PhD博士论文。这篇论文的深度学习章节提出了一种异常检测算法 (Recurrent Neural Network Anomaly Detection, ReNN AnD)，预测地质类型的改变。训练LSTM预测下一秒的时间序列数据，把预测值的偏差作为潜在的地质类型改变的提示。这个思路感觉和我们比赛自选课题相关。

# Methods
设备数据的处理，除了特征工程，大致可以看做时间序列表格数据的预测问题。例如必选课题一，就是对每一个循环的设备数据回归预测两个稳定段掘进控制参数，输入一个有三十个时间点的时间序列数据、输出回归值（两个数字）。

## 利用地质信息

时间序列里的参数都是连续量，但注意到不少地质特征是类别数据（categorical data）。类别数据就如字面意思，指的是和连续量（一个数字）相对的表示类别的数据，如编号或字符串表示的类别。用神经网络处理类别数据常用的方法是embedding。Embedding是one hot encoding的改进，用于把离散的类别数据转换为神经网络能处理的连续数据。Embedding将每个离散量映射至一个高维向量，因此可以捕捉离散量之间语义上的关系，而不被数字表示限制。 

给的地质信息是分段的，比如桩号xxxxx.xx-yyyyy.yy对应某种地质类型，但实际上地质显然不是突变的。是否有办法对类别数据插值？或者在embedding空间插值？插值以后，选择地址信息时，就不是预测序列所在分段，而是预测序列在全程所在的位置比例。 

### 模型

研讨会上展示了两个baseline，分别是中铁自行研发的、已进入生产环境的线性回归模型和某个大学（忘记了）的restricted Boltzmann machine。两个baseline模型都是传统的机器学习回归模型，而没有利用好时间序列数据的特点。时间序列是一种比较复杂的预测问题，和回归预测不同，时间序列更有序列前后依赖问题，因此baseline简单地手工构建数据在时间维度的联系，是不能产生最佳预测性能的。不论学术竞赛还是工业界，Recurrent Neural Networks (RNN)都因为能够自动学习到序列中的前后依赖关系，特别是擅长处理数据长期依赖关系的Long Short-Term Memory (LSTM)模型，广泛应用于时间序列上。因此，模型方面显然应该直接从LSTM作为我们的baseline开始来改进。但也可以尝试非recurrent的方法，例如QANet提出的使用本地卷积和全局self-attention替代recurrent的模型。非recurrent的方法通常训练更快，使得能在更多数据上训练更多次迭代，从而提高精度。

我认为预测赛题和自然语言处理的分类任务有很强相似性。任务都是输入一个序列的向量，输出一个数字。不同的是，我们预测的是连续量，而分类任务输出的是类别编号，但模型结构可以是大致相同的。模型可以着重参考语言模型的模型设计。 

几个模型想法： 

**模型一**

上面提到直接从LSTM作为baseline开始，但查资料的时候发现了一个LSTM的改进版[AWD-LSTM](https://yashuseth.blog/2018/09/12/awd-lstm-explanation-understanding-language-model/)。在自然语言处理（NLP）里，AWD-LSTM一直占据了语言模型的state-of-the-art。AWD-LSTM全称叫ASGD Weight-Dropped LSTM，相对原始LSTM的改进主要分两个部分，regularization和用ASGD训练。AWD-LSTM用了很多regularization技巧，例如DropConnect置零边权值、Embedding Dropout、重复使用Dropout Mask等等；ASGD是SGD的改进，可以提升收敛结果。 

模型一的输入输出相对简单，直接向模型一输入上升段序列，生成latent state，再用一个全连接网络转换成预测。模型一最简单，可以直接调库里的AWD-LSTM，但不能利用起地质信息。 

**模型二**

希望模型二能利用起来地质信息。训练数据中有序列对应的桩号，从而可以找到对应的地质数据。容易想到的比较简单地（naive）利用地质信息的方法是：模型二在预测时，首先向RNN输入序列生成latent state，用latent state猜测可能属于的地质区域，取回对应的地质信息。当然，这种简单方法还有需要探究的地方：一是RNN产生latent state的过程是不知道地质信息的，因此不知道如果把取回的地质信息和输入序列一道输入进RNN，是否能提升latent state的质量？这篇关于记忆的[神经科学](https://towardsdatascience.com/understanding-memory-in-deep-learning-systems-the-neuroscience-psychology-and-technology-78922ae6a1dd)博客提到，是否拥有一段记忆可能影响对一件事的认知，那么取回地质信息的记忆必然会影响预测结果，只是不知道是否需要把地质信息和输入序列同时重新输入RNN，生成latent state？又或许之后同时看见latent state和地质信息的全连接层足以提供足够的计算，获得最佳效果？两种策略都可以试试。二是不知道地质信息预测的错误对结果影响是否显著，这可能可以用插值的方法缓解。 

模型二在训练时属于多任务模型。既要把输入序列按照地质类别分类，又要回归预测最终结果。RNN在两个任务中是共用的，因此叠加loss相当于对RNN反复训练。地质信息的输回有“模型预测的”和“正确的”两个通路，训练时可以模仿teacher forcing，最开始可以不必向模型提供自己预测得的地质类别的信息，而直接输入正确对应的地质类别；达到一定准确度后，再按照概率切换回模型预测的地质信息通路。 

**模型三**

上面提到，QANet提出使用本地卷积和全局self-attention替代RNN来减少计算量、增加迭代次数对精度有更好正面影响，这也可以尝试的方向。 

模型的超参数主要有RNN层数和隐藏层维度、全连接层层数和隐藏层维度、embedding维度、dropout概率。不知道除了碰运气还能怎么选择。想试试[Neural Architecture Search](https://github.com/Microsoft/nni)。 

## 理解数据

### Feature Importance

Feature Importance定义为：一个特征重新排列后造成的预测准确度下降比例。一个特征重新排列后愈是损害预测精度，这个特征愈是重要。Feature Importance一般是机器学习项目模型建立好以后做的第一件事。用模型理解数据，才能更好优化模型。在[1]中，他们用训练好的Guided Regularized Random Forests (GRRFs)模型分析feature importance，可惜他们只用随机森林做特征选择，挑选出最重要的几个特征重新构建回归模型。 

### 冗余特征

冗余特征指重复出现的表示相似含义的特征。从随机森林的例子能学到，冗余特征可能损害模型性能，主要因为冗余特征导致冗余计算，模型重复做着相同的劳动，拖慢同等时间内的进步。不知道同等逻辑是否同样适用于LSTM模型，可以尝试移除冗余特征。移除冗余特征的常用方法是聚类分析（Cluster analysis）——计算特征间的相关性矩阵（Spearman’s R correlation matrix），按照相似性聚类。 

### Partial Dependence

特征间的相互作用关系。画[partial dependence plot](https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-4-a536f333b20d)。 



## Follow Me

在读的reference分享在了[Mendeley](https://www.mendeley.com/community/tbm-prediction-challenge) ；代码持续更新在[Github](https://github.com/TBM-prediction/ywx)。



[1] Maher, J. (n.d.). A Machine Learning Approach to Predicting and Maximizing Penetration Rates in Earth Pressure Balance Tunnel Boring Machines.

[2] Grima, M. A., Bruines, P. A., & Verhoef, P. N. W. (2000). Modeling Tunnel Boring Machine Performance by. *Space Technology*, *15*(3), 259–269. 

[3] Maher Jr., J. I. (n.d.). Improved Tunneling Knowledge Through Robust Machine Learning.

[4] Salimi, A., Moormann, C., Singh, T. N., & Jain, P. (2015). TBM Performance Prediction in Rock Tunneling Using Various Artificial Intelligence Algorithms. *11 Th Iranian and 2 Nd Regional Tunnell Ing Conference*, (November). 
